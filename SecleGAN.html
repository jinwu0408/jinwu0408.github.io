<!DOCTYPE html>
<html>

<head>
  <title>Secle GAN</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="shortcut icon" type="image/png" sizes="48x48" href="img/logo.png">
  <link href='https://fonts.googleapis.com/css?family=Varela+Round' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="css/Secle.css">
</head>

<body>
  <div class="banner secle_banner">
    <div class="banner_inner">
      <h1 class="shadow">Secle GAN</h1>
      <a href="#content"><img class="arrow" src="img/arrow.svg" alt="Down arrow"></a>
    </div>
  </div>
  <div class="context">
    <div class="primary-content indent" id="content">
      <div class="hori_bar_blue">
        <!--introduction-->
        <h1>Introduction</h1>
        <p>
            Nowadays, CycleGAN algorithm has been broadly applied to many different fields.
            For example, the algorithm has been used in the Medical diagnosis field to help
            with the recognition of Lesion and tumor because of its excellent performance
            augmentation, which it could recognize and pick the lesion part in a CT scan
            image. Moreover, image-to-image translation is another useful application of
            CycleGAN, which it could be used into image reconstruction. For example, Using
            CycleGAN, Jack Clark was able to convert ancient maps of Babylon and London into
            modern Google Map satellite views. What's more, by transforming the domain into
            target domain, CycleGAN could also render real word sceneries into computer
            graphics style. Therefore, CycleGAN has much potential that is worthwhile to
             further optimize and improve. Even Yann LeCun, the God father of the modern
              deep learning, once said that "GANs is the most interesting idea in the past
               10 years in machine learning". Therefore, we think diving deeper to the
               field of GANs, especially CycleGAN, is a meaningful try.
        </p>
      </div>
      <!--End introduction-->
      <!-- Metric-->
      <div class="hori_bar_blue">
        <!--Constraint-->
        <h1 class="title">Problem</h1>
          <div class="image">
            <img src="img/misclassify.png" alt="secle1">
            <figcaption>Fig.1 A misclassified example by CycleGAN</figcaption>
          </div>
          <p>However, even the finest jewelry will not be perfect, just like
            the CycleGAN algorithm. Although it has much potential and proved
            practical in real world applications. It could sometimes generate
            failed results. For example, when doing horses-to-zebras
            transformations, if a person is riding the horse, there is a chance
            that the person will also be transformed into zebra, thus resulting
            a failure case.</p>
            <p>The  above  result  is  definitely  not  the  one  that  we
              expect from CycleGAN. Therefore, we would like to rectifythis
              mistake generated by CycleGAN algorithm and hope tosuccessfully
              transform the horse into zebra without chang-ing the person at the
              same time.</p>
      </div>
<!-- End of Metric-->
  <!--Model-->
      <div class="hori_bar_blue clearfix">
        <h1 class="title">Related Work</h1>
          <h2 class="title">GAN</h2>
          <p>
            A generative adversarial network, also known as GAN, has two key
            components, the first component is what usually referred as generator
            network, which takes a random input and produce corresponding output
            sample, then together with the real input or sample label, they are
            fed into the second component of GAN model, a discriminator.
            Discriminator network would take the sample generated by the generator
            and the real sample to produce a binary classification result of
            either true or false, or real or fake, standing for whether the
            generator produce result that is similar to the real sample or not.
            If not, through backpropagation process, the discriminator would
            provide weights that the generator could use accordingly to generate
            samples that are more similar to the real sample.
          </p>
          <div class="image">
            <img src="img/GAN.png" alt="GAN">
            <figcaption>Fig.2 GAN architecture</figcaption>
          </div>

          <h2 class="title">Cycle GAN</h2>
          <p>
            As described above in Introduction, CycleGAN is aextension of GAN
            architecture. The CycleGAN   architecture   contains   two   separateGANs running in
            cycles. The two cycles are denoted as for-ward cycle and backward
            cycle. In forward cycle, the inputimage from domain A will be used to
            train the discriminatorA, then itâ€™s transformed into domain B, the
            target domain,by the generator A2B, which the result we denote as
            trans-formed B image.  Unlike the tradition GAN model whichconstantly
            update  the  generator  using  weights,  the  trans-formed B image
            will then be transformed back to domain Aby another generator B2A.
            And the result that comes out ofgenerator B2A is denoted as
            reconstructed A image.
          </p>
          <div class="image">
            <img src="img/CycleGAN_f.png" alt="CycleGAN_b">
            <figcaption>Fig.3 CycleGAN network architecture in forward cycle</figcaption>
          </div>
          <div class="image">
            <img src="img/CycleGAN_b.png" alt="CycleGAN_b">
            <figcaption>Fig.4 CycleGAN network architecture in forward cycle</figcaption>
          </div>
          <p>
            Lastly,the CycleGAN would compare the reconstructed A imagewith the
            original A image to compute the loss and updatethe model.In backward
            cycle, same logic is applied for the imagein domain B. Just like to
            forward cycle, backward processtransform  the  B  image  into  domain
            A,  then  transformedagain into domain B to acquire reconstructed B
            image.  Bycomparing  reconstructed  B  image  with  original  B
            image,the algorithm again compute the loss and update itself us-ing
            the loss information.
          </p>
      </div>
      <div class="hori_bar_blue clearfix">
        <h1 class="title">Secle GAN</h1>
          <h2 class="title">Structure</h2>
          <p>
            SecleGAN is our proposed method which combines thesemantic
            segmentation together with the traditional Cycle-GAN algorithm.
            The SecleGAN architecture is shown be-low.
          </p>
          <div class="image">
            <img src="img/SecleGAN2.png" alt="SecleGAN2">
            <figcaption>Fig.2 Secle GAN architecture</figcaption>
          </div>
          <p>Note that there are two different ways to implement ourSecleGAN
            algorithm.  First, we could feed the input image into the semantic
            segmentation block, which in our paperwe used U-net to perform the
            segmentation part.  Then af-ter the segmentation process, we would
            have the segmented instance, we then feed the instance into the
            CycleGAN algo-rithm to acquire the transformed target segmented
            instance.Finally, with the help of the mask, we are able to put
            thetransformed target segmented instance back into the origi-nal
            background as the output image.
          </p>
          <p>
            The other way would be that we could
            first feed the in-put image into the CycleGAN algorithm, and use
            the Cy-cleGAN to transform the entire input image into the other
            domain.   Often,  even  though  the  instance  is  successfully
            transformed into the target instance, the background content
            would be transformed as well.  Therefore, in order to avoidthis,
            we then use semantic segmentation technique to pullout the target
            instance,  and then apply mask to move the instance back to the
            original background content.   In thisway we could get a reconstructed
            image that has success-ful instance transformation and , at the same
            time, with theoriginal background content.  SecleGAN technique would
            prevent background information loss and could potentially
            have a huge impact because it greatly improve the overall
            performance of the traditional CycleGAN algorithm.
          </p>
      </div>
      <div class="hori_bar_blue clearfix">
        <h1 class="title">Experiment</h1>
          <h2 class="title">Dataset</h2>
          <p>
            To test out our proposed improvement, Secle GAN. We are using the
            dataset found on Kaggle. There are image dataset and segmentation
            masks labeled for ten different species of butterfly. The two
            kind of butterfly that we used in this experiment is "Danaus
            plexippus" and "Nymphalis antiopa", containing 65 and 100 images
            respectively. Note the difference between number of training data
            actually make a difference in our resulted generator models.
          </p>
          <div class="image">
            <img src="img/orange.jpeg" alt="orange">
            <figcaption>Fig.5 Danaus plexippus</figcaption>
          </div>
          <div class="image">
            <img src="img/black.png" alt="black">
            <figcaption>Fig.6 Nymphalis antiopa</figcaption>
          </div>
          <h2 class="title">Models and Parameters</h2>
          <p>
            In this experiment, we are implementing U-net before Cycle GAN.
            Similar result can be achieved with the reversed order. The images
            and segmentation masks are first used to train the U-net for it to
            learn how to segment butterfly out of the image. Next, the images
            are put into Cycle GAN to train. Then, we use the predict mask to
            segment out the background and the butterfly. Last, we put
            everything together and reconstruct the image.
          </p>
          <p>
            The parameters for Cycle GAN training involves lambda, which is the
            weights factor that balance between losses. However, in order to
            achieve better performance
            and explore, we have introduced an identity loss and weighted the
            three losses, instead of two. The three losses are
          </p>
          <p>After trails and evaluation with cross-validation, the weightsused
             in this experiment is [1,5,10], respectively. And Identity loss is
             computed by compare the original image from A and transformed images
             from A to A.
          </p>
          <p>In addition to weight factors, the learning rate is also important.
            We found out it is very easy to miss the local minimum/maximum if a
            larger step is taken. The loss function will bouncing around,
            instead of "monotonically" decreasing. After trails, we set the
            learning rate to $\eta=0.0002$ eventually. Note that the learning
            rate is small because the loss is very sensitive to learning rate.
          </p>
          <h2>Performance</h2>
          <p>
            As we mentioned above, U-net was trained the first. The end accuracy
            was around 0.97 after 20 epoches. The accuracy and loss curve is shown.
          </p>
          <div class="image">
            <img src="img/unet_loss.png" alt="unet_loss">
            <figcaption>Fig.7 U-net accuracy/loss Plot</figcaption>
          </div>
          <p>
            The performance of our Secle GAN algorithm is shown here. Note there
            the background was not transformed, only the butterfly. The result
            shown is at 200 epoches. The loss was not minimized yet. The result
            could be better, but we are limited in time and computational
            constraint. However, the result is optimising. We can see the orange
            butterfly is turning black, while the black butterfly is turning
            orange. In our dataset, there are more black butterfly data than
            orange butterfly, as mentioned above. Therefore the discriminator
            is trained better, hence the generator. We see this in our result.
            The overall performance of orange to black is better than the
            performance of black to orange. We see the effect of generator and
            discriminator compete with other, causing both to learn better.
          </p>

          <div class="image">
            <img src="img/AtoB_generated_plot_000200.png" alt="unet_loss">
            <figcaption>Fig.8 Resulf of A to B</figcaption>
          </div>
          <div class="image">
            <img src="img/BtoA_generated_plot_000200.png" alt="unet_loss">
            <figcaption>Fig.9 Resulf of B to A</figcaption>
          </div>
          <h2>Baselines Comparisons</h2>
          <p>
            The baseline that we used for compare and evaluate our solution is the
            original Cycle GAN. The result of Cycle GAN is shown in Figure 5 and
            Figure 6. We see the problem of the Cycle GAN, which the background
            got transformed. However, in our Secle GAN result, the background
            information is maintained. The images is more vivid, and the
            improvement is obvious. Therefore, Secle GAN is definitely a
            improvement upon Cycle GAN.
          </p>
      </div>
      <h1>Reflection and Future Improvement</h1>
      <p>
        We made one mistake in this experiment. The input size for Cycle GAN and
        U-net was not consistent. Input size was 128x128 for Cycle GAN, and
        U-net 256x256. This caused us troubles during reconstruction and made
        the resulting images blurry as we can see in Figure 5 and Figure 6.
      </p>
      <p>
        There are some places that can still be improved or application to be build upon.
      </p>
    </div><!-- End .primary-content -->
    <div class="resume secondary-content" id="resume">
      <h1>Paper</h1>
      <iframe src="Secle_GAN.pdf" width="100%" height="500px">
      </iframe>
    </div>
  </div>

  <footer>
    <ul>
      <li><a href="https://twitter.com/jinwu0408" target=_blank class="social twitter">Twitter</a></li>
      <li><a href="https://www.linkedin.com/in/jin-wu/" target=_blank class="social linkedin">LinkedIn</a></li>
      <li><a href="https://github.com/jinwu0408" target=_blank class="social github">Github</a></li>
      <li><a href="mailto:jinwu0408@gmail.com" target=_blank class="social google">Email</a></li>
      <li><a href="https://www.instagram.com/jin_wu/" target=_blank class="social instagram">Instagram</a></li>
    </ul>
    <p class="copyright">Copyright 2019, Jin Wu</p>
  </footer>
</body>

</html>
